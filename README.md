# production-ready-server-with-ansible-
This repository provides a complete example of how to build and operate a production‑ready server on AWS using Infrastructure as Code (Terraform) and configuration management (Ansible). It provisions the network, compute, storage and monitoring infrastructure, configures an Ubuntu host with hardened SSH, firewall rules and fail2ban, installs Docker & Nginx, deploys a containerised application via Docker Compose, sets up scheduled database backups to S3 and implements health‑check/monitoring with alerts. Secrets are managed securely through Ansible Vault and Terraform modules encapsulate reusable infrastructure components.

## Overview
At a high level, the stack works as follows:
  * **Infrastructure provisioning (Terraform)** – The ```infra/``` folder contains a top‑level module (```infra.tf```) that orchestrates several sub‑modules. It creates an S3 bucket with a lifecycle policy for backups, builds a VPC with public subnets, launches an EC2 instance with a minimal IAM role and security group, and sets up monitoring resources such as CloudWatch log groups and SNS topics. For example, the ```backups_storage``` module provisions an S3 bucket and attaches an IAM role/policy allowing the EC2 instance to write objects. The ```servers``` module creates a security group allowing HTTP/HTTPS/SSH traffic and an EC2 instance that uses the instance profile from the backups module. The ```monitoring``` module defines SNS topics and a CloudWatch CPU utilization alarm that sends notifications via email and attaches the CloudWatchAgentServerPolicy to the EC2 role.
  * **Server bootstrap & configuration (Ansible)** – Playbooks in the project root coordinate several roles under ```roles/```. site.yml bootstraps the server by running the common, security, docker and nginx roles. ```deploy.yml``` deploys the application with the app role and reads secrets from an encrypted vault. ```backup.yml``` schedules database backups via the backup role. ```healtcheck.yml``` configures periodic health checks via the monitoring role.
  * **Application deployment** – The **app** role ensures the project directory exists and synchronises your local application source into the EC2 instance using ```ansible.builtin.synchronize```, excluding virtual‑env and Git artefacts. It renders a ```.env``` file from a template and triggers ```docker compose``` via a handler to build and run the container. After deployment, it waits for the ```/health``` endpoint on the HTTPS URL to return 200.
  * **System hardening & packages** – The common role updates and upgrades apt packages, sets the system timezone, creates a deploy user with sudo privileges and installs base packages such as ```curl```, ```ufw```, ```fail2ban``` and ```unzip```. The security role locks down SSH by installing an sshd drop‑in, disabling root/password logins, switching to a non‑default port and asserting that the daemon listens on that port. It also manages UFW to rate‑limit SSH and enable the firewall and configures fail2ban. Key‑based authentication is enforced and keys can be generated automatically.
  * **Docker & reverse proxy** – The **docker** role installs Docker Engine and the Compose plugin from the official repository and ensures the daemon is running with the deploy user added to the ```docker``` group. The nginx role installs Nginx, opens ports 80/443 in the firewall, serves a static site while the SSL certificate is obtained and later provides a reverse proxy that terminates TLS and forwards traffic to the Dockerised application. HTTPS is enabled with Let’s Encryptusing the ```geerlingguy.certbot``` role; once the certificate is present, a HTTPS‑specific configuration is deployed.
  * **Backups** – The **backup** role creates a scripts directory, installs a templated shell script (```backup.sh```) that uses ```pg_dump``` to dump a PostgreSQL database from the ```db``` container and uploads the compressed dump to the configured S3 bucket using the AWS CLI. It also installs a cron entry under ```/etc/cron.d/db-backup``` to schedule this script. The S3 bucket in Terraform enforces a lifecycle rule to expire backups after seven days.
  * **Monitoring & alerting** – The **monitoring** role installs the CloudWatch Agent, writes a JSON configuration that streams container logs to a CloudWatch log group and ensures the agent is running. It also deploys a health‑check script that inspects the database container’s health status and publishes alerts to an SNS topic via the AWS CLI when the database is unavailable or unhealthy. A cron job runs this script periodically. Terraform sets up SNS topics for CPU alarms and health‑check notifications and grants the EC2 role permission to publish to SNS. An alarm monitors EC2 CPU utilisation and triggers the CPU SNS topic when the average CPU usage exceeds 80 %.

## Project structure
```.
├── infra/            # Terraform configuration (top‑level and modules)
├── inventory/        # Ansible inventory and group variables
├── roles/            # Reusable Ansible roles (app, backup, common, docker, nginx, security, monitoring)
├── templates/        # Jinja2 templates for config files, cron jobs, CloudWatch agent, etc.
├── site.yml          # Playbook: bootstrap server with common, security, docker and nginx roles
├── deploy.yml        # Playbook: deploy application (uses encrypted secrets)
├── backup.yml        # Playbook: schedule database backups
├── healtcheck.yml    # Playbook: configure health check monitoring
├── vars/             # Non‑secret Ansible variables (e.g. user)
└── vault/            # Encrypted Ansible Vault file for secrets (DB credentials, etc.)
```

## ```infra/``` and Terraform modules
The top‑level ```infra/infra.tf``` file declares the AWS provider and instantiates four modules: ```backups_storage```, ```network```, ```servers``` and ```monitoring```. Each module lives under ```infra/modules``` and can be reused independently:
  * **backups_storage** – Creates an S3 bucket for database backups, attaches a lifecycle policy to expire backups after 7 days and defines an IAM role + instance profile that allows the EC2 instance to upload objects. Outputs the instance profile name and role name for consumption by other modules.
  * **network** – Uses the official ```terraform-aws-modules/vpc``` module to create a VPC, two public subnets and NAT gateway(s) from the provided CIDR. It exports the VPC ID and public subnet IDs.
